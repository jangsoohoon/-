{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"colab":{"name":"01-딥러닝과 추천알고리즘-03-Neural Collaborative Filtering 실습.ipynb","provenance":[{"file_id":"1nAG9mn7f8YFOELV2UsbkEruQHOCXngCs","timestamp":1609776703964}],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"IF2lN7b3KxIM"},"source":["# Neural Collaborative Filtering\n","1. [논문](https://arxiv.org/pdf/1708.05031.pdf)\n","2. Keras로 작성된 [저자 코드](https://github.com/hexiangnan/neural_collaborative_filtering)\n","3. 논문은 0과 1로 user-item interaction으로 matrix을 나타내고 학습했으나, 이번 실습에서는 rating을 직접 예측하고, loss를 구해보는 것을 진행한다"]},{"cell_type":"markdown","metadata":{"id":"Vwo_-gxkMHvu"},"source":["## Configuration"]},{"cell_type":"code","metadata":{"id":"9BUxy0TqJpIZ"},"source":["import os\n","import pandas as pd\n","import numpy as np\n","from matplotlib import pyplot as plt\n","from sklearn.model_selection import train_test_split\n","\n","import math\n","from torch import nn, optim\n","import torch\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","\n","from tqdm import tqdm\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sd5EcJo-MNYF"},"source":["## Load Dataset\n","- KMRD 데이터셋 활용\n","- google colab의 경우 data path 다시 확인하기"]},{"cell_type":"code","metadata":{"id":"8Gnd5SY6MaQn"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_ddZzgIiMMZK"},"source":["data_path = '/content/drive/MyDrive/data/kmrd/kmr_dataset/datafile/kmrd-small'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5MzgHnp4JpIg"},"source":["def read_data(data_path):\n","  df = pd.read_csv(os.path.join(data_path,'rates.csv'))\n","  train_df, val_df = train_test_split(df, test_size=0.2, random_state=1234, shuffle=True)\n","  return train_df, val_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fqKIrB08qGGd"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VZ8oc1G-NYsO"},"source":["# 학습할 영화 데이터 분석\n","train_df, val_df = read_data(data_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DsohSsCHNeXx"},"source":["print(train_df.shape)\n","print(train_df.head())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e8qwa_0RNf_z"},"source":["val_df.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ut8hvxxRJpIg"},"source":["fig, ax = plt.subplots(1, 2, sharex='col', sharey='row', figsize=(12,7))\n","ax = ax.ravel()\n","\n","train_df['rate'].hist(ax=ax[0])\n","val_df['rate'].hist(ax=ax[1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ochDMXb5JpIi"},"source":["train_df['rate'].describe()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a7Py8f2EXJzv"},"source":["## Load movie dataframe"]},{"cell_type":"code","metadata":{"id":"IkPeoquPXNvp"},"source":["# Load all related dataframe\n","movies_df = pd.read_csv(os.path.join(data_path, 'movies.txt'), sep='\\t', encoding='utf-8')\n","movies_df = movies_df.set_index('movie')\n","\n","castings_df = pd.read_csv(os.path.join(data_path, 'castings.csv'), encoding='utf-8')\n","countries_df = pd.read_csv(os.path.join(data_path, 'countries.csv'), encoding='utf-8')\n","genres_df = pd.read_csv(os.path.join(data_path, 'genres.csv'), encoding='utf-8')\n","\n","# Get genre information\n","genres = [(list(set(x['movie'].values))[0], '/'.join(x['genre'].values)) for index, x in genres_df.groupby('movie')]\n","combined_genres_df = pd.DataFrame(data=genres, columns=['movie', 'genres'])\n","combined_genres_df = combined_genres_df.set_index('movie')\n","\n","# Get castings information\n","castings = [(list(set(x['movie'].values))[0], x['people'].values) for index, x in castings_df.groupby('movie')]\n","combined_castings_df = pd.DataFrame(data=castings, columns=['movie','people'])\n","combined_castings_df = combined_castings_df.set_index('movie')\n","\n","# Get countries for movie information\n","countries = [(list(set(x['movie'].values))[0], ','.join(x['country'].values)) for index, x in countries_df.groupby('movie')]\n","combined_countries_df = pd.DataFrame(data=countries, columns=['movie', 'country'])\n","combined_countries_df = combined_countries_df.set_index('movie')\n","\n","movies_df = pd.concat([movies_df, combined_genres_df, combined_castings_df, combined_countries_df], axis=1)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z6yE06GxYV3W"},"source":["movies_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qkIRJboJq-8j"},"source":["- 논문:  user latent vector + item latent vector\n","- 새롭게 생각할 수 있는 방법: user latent vector + item latent vector + etc vector (예시) meta information "]},{"cell_type":"code","metadata":{"id":"cSfHpEHpJpIi"},"source":["# 영화 데이터의 메타 정보를 확인한다\n","movieName_dict = movies_df.to_dict()['title']\n","genres_dict = movies_df.to_dict()['genres']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IvnmEfzMJpIj"},"source":["movies_df['genres']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TygbVbUaJpIl"},"source":["## Dataset Loader "]},{"cell_type":"code","metadata":{"id":"E3bh5SEJJpIl"},"source":["class DatasetLoader:\n","    def __init__(self, data_path):\n","        self.train_df, val_temp_df = read_data(data_path)\n","\n","        self.min_rating = min(self.train_df.rate)\n","        self.max_rating = self.train_df.rate.max()\n","\n","        self.unique_users = self.train_df.user.unique()\n","        self.num_users = len(self.unique_users)\n","        self.user_to_index = {original: idx for idx, original in enumerate(self.unique_users)}\n","        # 0 1 0 0 0 ... 0\n","\n","        self.unique_movies = self.train_df.movie.unique()\n","        self.num_movies = len(self.unique_movies)\n","        self.movie_to_index = {original: idx for idx, original in enumerate(self.unique_movies)}\n","\n","        self.val_df = val_temp_df[val_temp_df.user.isin(self.unique_users) & val_temp_df.movie.isin(self.unique_movies)]\n","\n","    def generate_trainset(self):\n","        # user 0, 0, 0, 1,2, 3,3, -> movie: 0,0,0,0,0,0,\n","        X_train = pd.DataFrame({'user': self.train_df.user.map(self.user_to_index),\n","                     'movie': self.train_df.movie.map(self.movie_to_index)})\n","        y_train = self.train_df['rate'].astype(np.float32)\n","\n","        return X_train, y_train\n","\n","    def generate_valset(self):\n","        X_val = pd.DataFrame({'user': self.val_df.user.map(self.user_to_index),\n","                              'movie': self.val_df.movie.map(self.movie_to_index)})\n","        y_val = self.val_df['rate'].astype(np.float32)\n","        return X_val, y_val"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ep1DLtpHJpIm"},"source":["## Model Structure\n","- 논문에서 제시한 모델 구조를 그대로 구현하고 영화 데이터로 실습해본다. \n","- User Vector는 전체 영화 데이터에서 영화를 평가한 userid를 onehot vector로 나타낸 형태\n","- Item Vector는 전체 영화 데이터에 등장하는 영화의 id를 onehot vector로 나타낸 형태\n","\n"]},{"cell_type":"code","metadata":{"id":"1QAZm2wQJpIm"},"source":["class FeedForwardEmbedNN(nn.Module):\n","\n","    def __init__(self, n_users, n_movies, hidden, dropouts, n_factors, embedding_dropout):\n","        super().__init__()\n","        self.user_emb = nn.Embedding(n_users, n_factors)\n","        self.movie_emb = nn.Embedding(n_movies, n_factors)\n","        self.drop = nn.Dropout(embedding_dropout)\n","        self.hidden_layers = nn.Sequential(*list(self.generate_layers(n_factors*2, hidden, dropouts)))\n","        self.fc = nn.Linear(hidden[-1], 1)\n","\n","    def generate_layers(self, n_factors, hidden, dropouts):\n","        assert len(dropouts) == len(hidden)\n","\n","        idx = 0\n","        while idx < len(hidden):\n","            if idx == 0:\n","                yield nn.Linear(n_factors, hidden[idx])\n","            else:\n","                yield nn.Linear(hidden[idx-1], hidden[idx])\n","            yield nn.ReLU()\n","            yield nn.Dropout(dropouts[idx])\n","\n","            idx += 1\n","\n","    def forward(self, users, movies, min_rating=0.5, max_rating=5):\n","        concat_features = torch.cat([self.user_emb(users), self.movie_emb(movies)], dim=1)\n","        x = F.relu(self.hidden_layers(concat_features))\n","        # 0과 1사이의 숫자로 나타낸다\n","        out = torch.sigmoid(self.fc(x))\n","        # rating으로 변환한다\n","        out = (out * (max_rating - min_rating)) + min_rating\n","\n","        return out\n","\n","    def predict(self, users, movies):\n","        # return the score\n","        output_scores = self.forward(users, movies)\n","        return output_scores"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bmy6566FJpIn"},"source":["class BatchIterator:\n","\n","    def __init__(self, X, y, batch_size=32, shuffle=True):\n","        X, y = np.asarray(X), np.asarray(y)\n","\n","        if shuffle:\n","            index = np.random.permutation(X.shape[0])\n","            X, y = X[index], y[index]\n","\n","        self.X = X\n","        self.y = y\n","        self.batch_size = batch_size\n","        self.shuffle = shuffle\n","        self.n_batches = int(math.ceil(X.shape[0] // batch_size))\n","        self._current = 0\n","\n","    def __iter__(self):\n","        return self\n","\n","    def __next__(self):\n","        return self.next()\n","\n","    def next(self):\n","        if self._current >= self.n_batches:\n","            raise StopIteration()\n","        k = self._current\n","        self._current += 1\n","        bs = self.batch_size\n","        return self.X[k * bs:(k + 1) * bs], self.y[k * bs:(k + 1) * bs]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qbiX6uwuJpIn"},"source":["def batches(X, y, bs=32, shuffle=True):\n","    for x_batch, y_batch in BatchIterator(X, y, bs, shuffle):\n","        x_batch = torch.LongTensor(x_batch)\n","        y_batch = torch.FloatTensor(y_batch)\n","        yield x_batch, y_batch.view(-1, 1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7R6I8wWjJpIn"},"source":["## Train model\n","데이터셋과 모델 학습에 필요한 configuration을 입력하고, 학습을 하는 함수를 만든다\n","configuration을 바꾸면서 모델의 성능을 측정해볼 수 있다. "]},{"cell_type":"code","metadata":{"id":"6stv_nRnJpIo"},"source":["def model_train(ds, config):\n","    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","    X_train, y_train = ds.generate_trainset()\n","    X_valid, y_valid = ds.generate_valset()\n","    print(f'TrainSet Info: {ds.num_users} users, {ds.num_movies} movies')\n","\n","    model = FeedForwardEmbedNN(\n","        n_users=ds.num_users, n_movies=ds.num_movies,\n","        n_factors=config['num_factors'], hidden=config['hidden_layers'],\n","        embedding_dropout=config['embedding_dropout'], dropouts=config['dropouts']\n","    )\n","    model.to(device)\n","\n","    batch_size = config['batch_size']\n","    num_epochs = config['num_epochs']\n","    max_patience = config['total_patience']\n","    num_patience = 0\n","    best_loss = np.inf\n","\n","    criterion = nn.MSELoss(reduction='sum')\n","    criterion.to(device)\n","    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n","\n","    result = dict()\n","    for epoch in tqdm(range(num_epochs)):\n","        training_loss = 0.0\n","        for batch in batches(X_train, y_train, shuffle=True, bs=batch_size):\n","            x_batch, y_batch = [b.to(device) for b in batch]\n","            optimizer.zero_grad()\n","            # with torch.no_grad() 와 동일한 syntax 입니다\n","            with torch.set_grad_enabled(True):\n","                outputs = model(x_batch[:, 0], x_batch[:, 1], ds.min_rating, ds.max_rating)\n","                loss = criterion(outputs, y_batch)\n","                loss.backward()\n","                optimizer.step()\n","            training_loss += loss.item()\n","        result['train'] = training_loss / len(X_train)\n","\n","        # Apply Early Stopping criteria and save best model params\n","        val_outputs = model(torch.LongTensor(X_valid.user.values).to(device),\n","                            torch.LongTensor(X_valid.movie.values).to(device), ds.min_rating, ds.max_rating)\n","        val_loss = criterion(val_outputs.to(device), torch.FloatTensor(y_valid.values).view(-1, 1).to(device))\n","        result['val'] = float((val_loss / len(X_valid)).data)\n","\n","        if val_loss < best_loss:\n","            print('Save new model on epoch: %d' % (epoch + 1))\n","            best_loss = val_loss\n","            result['best_loss'] = val_loss\n","            torch.save(model.state_dict(), config['save_path'])\n","            num_patience = 0\n","        else:\n","            num_patience += 1\n","\n","        print(f'[epoch: {epoch+1}] train: {result[\"train\"]} - val: {result[\"val\"]}')\n","\n","        if num_patience >= max_patience:\n","            print(f\"Early Stopped after epoch {epoch+1}\")\n","            break\n","\n","    return result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fawHamk_JpIo"},"source":["def model_valid(user_id_list, movie_id_list, data_path):\n","    dataset = DatasetLoader(data_path)\n","    processed_test_input_df = pd.DataFrame({\n","        'user_id': [dataset.user_to_index[x] for x in user_id_list],\n","        'movie_id': [dataset.movie_to_index[x] for x in movie_id_list]\n","    })\n","\n","    # 학습한 모델 load하기 \n","    my_model = FeedForwardEmbedNN(dataset.num_users, dataset.num_movies,\n","                       config['hidden_layers'], config['dropouts'], config['num_factors'], config['embedding_dropout'])\n","    my_model.load_state_dict(torch.load('params.data'))\n","    prediction_outputs = my_model.predict(users=torch.LongTensor(processed_test_input_df.user_id.values),\n","                     movies=torch.LongTensor(processed_test_input_df.movie_id.values))\n","\n","    return prediction_outputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"liavZI4zJpIp"},"source":["dataset = DatasetLoader(data_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1xEzqD2cJpIp"},"source":["config = {\n","  \"num_factors\": 16,\n","  \"hidden_layers\": [64, 32, 16],\n","  \"embedding_dropout\": 0.05,\n","  \"dropouts\": [0.3, 0.3, 0.3],\n","  \"learning_rate\": 1e-3,\n","  \"weight_decay\": 1e-5,\n","  \"batch_size\": 8,\n","  \"num_epochs\": 3,\n","  \"total_patience\": 30,\n","  \"save_path\": \"params.data\"\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9SFyhQr-JpIp"},"source":["model_train(dataset, config)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"66PToOALWUZB"},"source":["val_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ukvOFg52JpIq"},"source":["movie_id_list = [10253, 10102, 10007]\n","user_id = 11242\n","user_id_list = [user_id] * len(movie_id_list)\n","pred_results = [float(x) for x in model_valid(user_id_list, movie_id_list, data_path)]\n","\n","result_df = pd.DataFrame({\n","    'userId': user_id_list,\n","    'movieId': movie_id_list,\n","    # 'movieName': [movieName_dict[x] for x in movie_id_list],\n","    # 'genres': [genres_dict[x] for x in movie_id_list],\n","    'pred_ratings': pred_results\n","})\n","\n","result_df.sort_values(by='pred_ratings', ascending=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_bZZOMwCvvUL"},"source":[""],"execution_count":null,"outputs":[]}]}